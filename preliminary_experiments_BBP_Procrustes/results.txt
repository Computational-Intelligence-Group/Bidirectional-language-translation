============================================================
STEP 1: Loading embeddings
============================================================
Loading embeddings from wiki.en.vec ...
  Detected header: 2519370 words, 300 dimensions
  Loaded 200000 words, dimension=300
Loading embeddings from wiki.es.vec ...
  Detected header: 985667 words, 300 dimensions
  Loaded 200000 words, dimension=300

============================================================
STEP 2: Normalizing vectors to unit length
============================================================
  Sample source norms after normalization: [1.         1.         1.         0.99999994 1.        ]

============================================================
STEP 3: Loading bilingual dictionaries
============================================================
Training dictionary:
  Loaded 11977 pairs, skipped 0 (OOV)
Test dictionary:
  Loaded 2975 pairs, skipped 0 (OOV)

  Training pairs: 11977
  Test pairs: 2975
  Source dim: 300, Target dim: 300

============================================================
METHOD 1: Mikolov (2013) Linear Transform (MSE, unnormalized)
  Objective: min_W sum ||Wx_i - z_i||^2
  Using UNNORMALIZED embeddings (as in original paper)
============================================================
  Training time: 1.178s

  Forward (EN -> ES):
  P@1: 3.80% (113/2975)
  P@5: 16.50% (491/2975)

  Backward (ES -> EN) via W^T:
  P@1: 5.18% (154/2975)
  P@5: 18.32% (545/2975)

============================================================
METHOD 2: Xing (2015) Orthogonal Transform - Procrustes (closed-form)
  Objective: max_W sum (Wx_i)^T z_i  s.t. W^TW = I
  Using NORMALIZED embeddings
============================================================
  Training time: 0.086s
  Orthogonality check ||W^TW - I||_F = 9.285431e-06

  Forward (EN -> ES):
  P@1: 38.86% (1156/2975)
  P@5: 67.50% (2008/2975)

  Backward (ES -> EN) via W^T (free from orthogonality):
  P@1: 51.36% (1528/2975)
  P@5: 73.34% (2182/2975)

============================================================
METHOD 3: Xing (2015) Orthogonal Transform - Iterative
  lr=1.0, n_iters=100
  Gradient ascent + SVD re-orthogonalization each step
============================================================
  Iter 10/100: objective = 7577.5205
  Iter 20/100: objective = 7577.5430
  Iter 30/100: objective = 7577.5439
  Iter 40/100: objective = 7577.5483
  Iter 50/100: objective = 7577.5454
  Iter 60/100: objective = 7577.5435
  Iter 70/100: objective = 7577.5474
  Iter 80/100: objective = 7577.5469
  Iter 90/100: objective = 7577.5444
  Iter 100/100: objective = 7577.5449
  Training time: 8.895s
  Orthogonality check ||W^TW - I||_F = 9.453521e-06

  Forward (EN -> ES):
  P@1: 38.86% (1156/2975)
  P@5: 67.50% (2008/2975)

  Backward (ES -> EN) via W^T:
  P@1: 51.36% (1528/2975)
  P@5: 73.34% (2182/2975)

============================================================
SUMMARY
============================================================
Method 1 (Mikolov): MSE loss, unnormalized, unconstrained W
Method 2 (Procrustes): Cosine loss, normalized, orthogonal W (closed-form)
Method 3 (Xing iter): Cosine loss, normalized, orthogonal W (iterative)

Methods 2 and 3 should give nearly identical results.
Both should outperform Method 1 (as shown in Xing Table 1 vs 2).

Saved W matrices: W_procrustes.npy, W_xing_iterative.npy, W_mikolov.npy
